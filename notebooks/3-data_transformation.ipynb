{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PoxkFVyPmKMJ"
   },
   "source": [
    "# 3.0 - Data Transformation\n",
    "This notebook is for creating one/multiple datasets with different feature subsets and transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 295223,
     "status": "ok",
     "timestamp": 1703770251981,
     "user": {
      "displayName": "Nischa564",
      "userId": "09387795752635677771"
     },
     "user_tz": -60
    },
    "id": "iF1MPPTSmVVY",
    "outputId": "42416b57-c726-42e0-9e49-361df2b73313"
   },
   "source": [
    "## Imports and loading\n",
    "Import necessary packages and load the preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    ! git clone https://github.com/nischa564/wind-speed-analysis.git # clone repository for colab\n",
    "    ! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import pykalman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load processed file\n",
    "df = pd.read_csv('wind-speed-analysis/data/processed/processed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Unwanted Features\n",
    "Often only a subset of features is required. So delete the rest of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns by names\n",
    "#columns_to_drop = ['<Column1>', '<Column3>']\n",
    "#df = df.drop(columns=columns_to_drop, axis=1)\n",
    "\n",
    "# Drop columns by index\n",
    "#index = 0\n",
    "#df = df.drop(df.columns[index], axis=1)\n",
    "\n",
    "# Drop columns by index range\n",
    "#start_index = 0\n",
    "#end_index = 1\n",
    "#df = df.drop(df.columns[start_index:end_index + 1], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "Normalization scales data to a standard range, usually between 0 and 1. It is useful when the features have different scales and ensures that all features contribute equally to the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and select the columns which should be normalized\n",
    "#cols = ['<Column 1>', '<Column 3>']\n",
    "cols = list(df.columns)\n",
    "\n",
    "# Create a MinMaxScaler object\n",
    "scaler = MinMaxScaler()\n",
    "    \n",
    "# Fit the scaler to the selected columns and transform them\n",
    "data_normalized = scaler.fit_transform(df[cols].values)\n",
    "    \n",
    "# Convert the normalized data to a pandas dataframe\n",
    "df_normalized = pd.DataFrame(data_normalized, index=df.index, columns=cols)\n",
    "    \n",
    "# Concatenate the normalized columns with the unnormalized columns\n",
    "df_untransformed = df[[col for col in df.columns if col not in cols]]\n",
    "df = pd.concat([df_untransformed, df_normalized], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization\n",
    "Standardization transforms data to have a mean of 0 and a standard deviation of 1. It is effective when features have different scales and you can assume a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and select the columns which should be standardized\n",
    "#cols = ['<Column 1>', '<Column 3>']\n",
    "cols = list(df.columns)\n",
    "\n",
    "# Create a StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "    \n",
    "# Fit the scaler to the selected columns and transform them\n",
    "data_standardized = scaler.fit_transform(df[cols].values)\n",
    "    \n",
    "# Convert the standardized data to a pandas dataframe\n",
    "df_standardized = pd.DataFrame(data_standardized, index=df.index, columns=cols)\n",
    "    \n",
    "# Concatenate the standardized columns with the unselected columns\n",
    "df_untransformed = df[[col for col in df.columns if col not in cols]]\n",
    "df = pd.concat([df_untransformed, df_standardized], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA\n",
    "PCA is a dimensionality reduction technique that transforms data into a new set of uncorrelated variables (principal components). It is used to capture the most significant variability in the data while reducing its dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and select the columns on which the pca is applied\n",
    "#cols = ['<Column 1>', '<Column 3>']\n",
    "cols = list(df.columns)\n",
    "\n",
    "# Fit the PCA and transform on the selected columns\n",
    "pca = PCA(n_components=2)\n",
    "data_pca = pca.fit_transform(df[cols])\n",
    "\n",
    "# Define a new name for the new features\n",
    "feature_name = 'pca_feature'\n",
    "\n",
    "# Convert the PCA data to a pandas dataframe\n",
    "new_cols = [f'{feature_name}_' + str(i+1) for i in range(data_pca.shape[1])]\n",
    "df_pca = pd.DataFrame(data_pca, columns=new_cols, index=df.index)\n",
    "\n",
    "# Concatenate the pca columns with the unselected columns\n",
    "df_untransformed = df[[col for col in df.columns if col not in cols]]\n",
    "df = pd.concat([df_pca, df_untransformed], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shifting\n",
    "Shifting involves moving data points by a constant value. It is used for various purposes, such as aligning signals or adjusting time series for temporal considerations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the number of periods to be shifted\n",
    "periods = 1\n",
    "# Select if you want to have all shifts between 1 and periods as own column\n",
    "multi_shift = True\n",
    "    \n",
    "# Initialize an empty dataframe to store the windowed data\n",
    "df_shifted = pd.DataFrame()\n",
    "    \n",
    "# Loop through each column in the dataframe and create the requested shifts for the specified columns\n",
    "for col in df.columns:\n",
    "    if col in cols:\n",
    "        if multi_shift:\n",
    "            for i in range(1, periods+1):\n",
    "                # define the name for the shifted column\n",
    "                shifted_col_name = col + '_shifted_' + str(i)\n",
    "                df_shifted[shifted_col_name] = df[col].shift(i)\n",
    "        else:\n",
    "            # define the name for the shifted column\n",
    "            shifted_col_name = col + '_shifted_' + str(periods)\n",
    "            df_shifted[shifted_col_name] = df[col].shift(periods)\n",
    "    \n",
    "# Convert the shifted data to a pandas dataframe\n",
    "new_cols = list(df_shifted.columns)\n",
    "df = pd.concat([df_shifted, df], axis=1)\n",
    "\n",
    "# Fill na values which are created during the process\n",
    "df = df.backfill()\n",
    "df = df.ffill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sliding Window\n",
    "A Sliding Window extracts subsets of data points sequentially, creating a \"window\" that moves through the dataset. It is used for tasks like feature extraction or smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and select the columns on which the sliding window is applied\n",
    "#cols = ['<Column 1>', '<Column 3>']\n",
    "cols = list(df.columns)\n",
    "\n",
    "# Selet a window size\n",
    "window_size = 2\n",
    "# Select between 'sum', 'mean', 'median', 'min', 'max', 'std' \n",
    "operation = 'mean' \n",
    "    \n",
    "# Initialize an empty dataframe to store the windowed data\n",
    "df_windowed = pd.DataFrame()\n",
    "    \n",
    "# Iterate over each column in the dataframe\n",
    "for col in cols:\n",
    "        \n",
    "    # Apply the specified operations to the column using a rolling window\n",
    "    if 'sum' == operation:\n",
    "        df_windowed[f'{col}_sum'] = df[col].rolling(window_size).sum()\n",
    "    elif 'mean' == operation:\n",
    "        df_windowed[f'{col}_mean'] = df[col].rolling(window_size).mean()\n",
    "    elif 'median' == operation:\n",
    "        df_windowed[f'{col}_median'] = df[col].rolling(window_size).median()\n",
    "    elif 'min' == operation:\n",
    "        df_windowed[f'{col}_min'] = df[col].rolling(window_size).min()\n",
    "    elif 'max' == operation:\n",
    "        df_windowed[f'{col}_max'] = df[col].rolling(window_size).max()\n",
    "    elif 'std' == operation:\n",
    "        df_windowed[f'{col}_std'] = df[col].rolling(window_size).std()\n",
    "        \n",
    "# Add the windowed data to the windowed dataframe\n",
    "new_cols = list(df_windowed.columns)\n",
    "df_windowed = pd.concat([df_windowed, df], axis=1)\n",
    "\n",
    "# Drop old columns\n",
    "df = df_windowed.drop(cols, axis=1)\n",
    "\n",
    "# Fill na values which are created during the process\n",
    "df = df.backfill()\n",
    "df = df.ffill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differencing\n",
    "It calculates the difference between consecutive data points. It is often used to transform a time series into a stationary series for trend and seasonality removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and select the columns on which the sliding window is applied\n",
    "#cols = ['<Column 1>', '<Column 3>']\n",
    "cols = list(df.columns)\n",
    "\n",
    "periods = 1\n",
    "    \n",
    "# Initialize an empty dataframe to store the windowed data\n",
    "df_diff = pd.DataFrame()\n",
    "    \n",
    "# Iterate over each column in the dataframe\n",
    "for col in cols:\n",
    "    # Apply the specified operations to the column using a rolling window\n",
    "    df_diff[f'{col}_diff'] = df[col].diff(periods=periods)\n",
    "    \n",
    "# Add the windowed data to the windowed dataframe\n",
    "new_cols = list(df_diff.columns)\n",
    "df_diff = pd.concat([df_diff, df], axis=1)\n",
    "\n",
    "# Drop old columns\n",
    "df = df_diff.drop(cols, axis=1)\n",
    "\n",
    "# Fill na values which are created during the process\n",
    "df = df.backfill()\n",
    "df = df.ffill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kalman Filter\n",
    "A Kalman Filter estimates the state of a dynamic system from a series of noisy measurements. It is widely used in signal processing, control systems and sensor fusion applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and select the columns on which the kalman filter is applied\n",
    "#cols = ['<Column 1>', '<Column 3>']\n",
    "cols = list(df.columns)\n",
    "\n",
    "# Initialize the Kalman filter\n",
    "kf = pykalman.KalmanFilter()\n",
    "    \n",
    "# Create an empty dataframe to store the filtered data\n",
    "df_filtered = pd.DataFrame(index=df.index)\n",
    "    \n",
    "# Iterate over each column to be filtered\n",
    "for col in cols:\n",
    "        \n",
    "    # Get the time series data as a numpy array\n",
    "    data = df[col].values\n",
    "        \n",
    "    # Apply the Kalman filter to the data\n",
    "    data_filtered, _ = kf.filter(data)\n",
    "        \n",
    "    # Add the filtered data to the filtered dataframe\n",
    "    df_filtered[col] = data_filtered\n",
    "        \n",
    "# Concatenate the pca columns with the unselected columns\n",
    "df_untransformed = df[[col for col in df.columns if col not in cols]]\n",
    "df = pd.concat([df_filtered, df_untransformed], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Preprocessed Dataset\n",
    "Save the transformed data in a new file. Rename if you need multiple files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('wind-speed-analysis/data/transformed/transformed.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN4Pvx8rIJWWog9ZJOkf8EJ",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "template_env",
   "language": "python",
   "name": "template_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
