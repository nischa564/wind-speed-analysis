{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PoxkFVyPmKMJ"
   },
   "source": [
    "# 2.0 Data Preprocessing\n",
    "This notebook allows to clean the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 295223,
     "status": "ok",
     "timestamp": 1703770251981,
     "user": {
      "displayName": "Nischa564",
      "userId": "09387795752635677771"
     },
     "user_tz": -60
    },
    "id": "iF1MPPTSmVVY",
    "outputId": "42416b57-c726-42e0-9e49-361df2b73313"
   },
   "source": [
    "## Imports and loading\n",
    "Import necessary packages and load the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from dateutil import parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv file\n",
    "df = pd.read_csv('../data/raw/<your_data>.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Unwanted Features\n",
    "If you already know you don't need certain features, remove them before the preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns by names\n",
    "#columns_to_drop = ['<Column1>', '<Column3>']\n",
    "#df = df.drop(columns=columns_to_drop, axis=1)\n",
    "\n",
    "# Drop columns by index\n",
    "#index = 0\n",
    "#df = df.drop(df.columns[index], axis=1)\n",
    "\n",
    "# Drop columns by index range\n",
    "#start_index = 0\n",
    "#end_index = 1\n",
    "#df = df.drop(df.columns[start_index:end_index + 1], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Categorical Features\n",
    "The further preprocessing requires only numeric features (no strings). So convert all categorical to numeric features before continuing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding\n",
    "Label Encoding is suitable when the categorical values have an ordinal relationship, meaning there is a meaningful order among the categories. Each category is assigned a unique numerical label. The labels are often assigned in ascending order based on their alphabetical or numerical order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns which should be encoded\n",
    "cols_cat = ['<Column 1>', '<Column 3>']\n",
    "\n",
    "\n",
    "# Loop through each categorical column to perform label encoding\n",
    "for i in cols_cat:\n",
    "    # Step 1: Store the original column values\n",
    "    original = df[i]\n",
    "\n",
    "    # Step 2: Create a mask for missing values in the column\n",
    "    mask = df[i].isnull()\n",
    "\n",
    "    # Step 3: Perform label encoding on the column and replace the original values\n",
    "    df[i] = LabelEncoder().fit_transform(df[i].astype(str))\n",
    "\n",
    "    # Step 4: Replace the encoded values with original values for missing values\n",
    "    df[i] = df[i].where(~mask, original)\n",
    "\n",
    "    # Step 5: Convert the column back to integers, treating 'nan' as NaN\n",
    "    df[i] = df[i].apply(lambda x: int(x) if str(x) != 'nan' else np.nan) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding\n",
    "One-Hot Encoding is suitable when the categorical values are nominal, meaning there is no inherent order among the categories. Each category is represented by a binary column (0 or 1) in a new matrix. The column corresponding to the category is marked with a 1, and others are marked with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and select the columns which should be encoded\n",
    "cols_cat = ['<Column 1>', '<Column 3>']\n",
    "\n",
    "# Define the One Hot Encoder\n",
    "encoder = OneHotEncoder(drop='first', handle_unknown='ignore')\n",
    "\n",
    "# Encode the selected columns\n",
    "df_cat_encoded = encoder.fit_transform(df_cat.astype(str)).toarray()\n",
    "\n",
    "# Save the result in a dataframe\n",
    "df_encoded = pd.DataFrame(df_cat_encoded, index=df_cat.index, columns=encoder.get_feature_names_out(df_cat.columns))\n",
    "\n",
    "# Delete the old features\n",
    "df = df.drop(cols_cat, axis=1)\n",
    "\n",
    "# Concat the onehot features back to the data \n",
    "df = pd.concat([df, df_encoded], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date Encoding\n",
    "Encode cyclic data using sine and cosine functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and select the columns which should be encoded\n",
    "#date_cols = ['<Column 1>']\n",
    "date_cols = ['sepal.width']\n",
    "\n",
    "for col in date_cols:\n",
    "    # Parse the date format\n",
    "    df[col] = df[col].apply(lambda x: parser.parse(x) if isinstance(x, str) else x)\n",
    "\n",
    "    # Encode year linearly\n",
    "    df[col + ' year'] = df[col].dt.year\n",
    "\n",
    "    # Encode other components using sine and cosine functions\n",
    "    components = ['month', 'day', 'hour', 'minute', 'second', 'microsecond']\n",
    "    for comp in components:\n",
    "        df[col + ' ' + comp + ' sin'] = np.sin(2 * math.pi * df[col].dt.__getattribute__(comp) / df[col].dt.__getattribute__(comp).max())\n",
    "        df[col + ' ' + comp + ' cos'] = np.cos(2 * math.pi * df[col].dt.__getattribute__(comp) / df[col].dt.__getattribute__(comp).max())\n",
    "\n",
    "# Remove the original date columns\n",
    "df.drop(date_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Already Numeric\n",
    "Some columns consist already of numeric values. Just convert them to numeric values. If the decimal numbers use the german writing, replace the comma with points before converting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_column_comma_and_set_type_float(col: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Converts a Pandas Series containing numeric strings with commas to float values.\n",
    "\n",
    "    Parameters:\n",
    "    - col: The input Pandas Series containing numeric strings.\n",
    "\n",
    "    Returns:\n",
    "    - The converted Pandas Series with values converted to float.\n",
    "    \"\"\"\n",
    "    # Use the map function to apply the specified lambda function to each element in the column\n",
    "    col = col.map(lambda x: x.replace('.', '0.0').replace(',', '.') if type(x) != float else x)\n",
    "\n",
    "    # Convert the column to the float type\n",
    "    col = col.astype(float)\n",
    "\n",
    "    # Return the converted column\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns which should be encoded\n",
    "cols_cat = ['<Column 1>', '<Column 3>']\n",
    "\n",
    "# Loop through each selected categorical column\n",
    "for i in cols_cat:\n",
    "    # Check if the column contains '.' or ',' in any of its values\n",
    "    if df[i].str.contains('.').any() or df[i].str.contains(',').any():\n",
    "        # If yes, apply the custom function to convert the column to float\n",
    "        df[i] = convert_column_comma_and_set_type_float(df[i])\n",
    "    else:\n",
    "        # If no '.', ',' found, use pd.to_numeric to convert the column to numeric\n",
    "        df[i] = pd.to_numeric(df[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill Missing Values\n",
    "Further operations require a dataset without missing values. So fill all missing values before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the imputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "# Apply the imputation to the dataset\n",
    "df = df.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect and Remove Outlier\n",
    "Outlier detection is important in various fields and applications because outliers, which are data points that significantly differ from the majority of the data, can have a significant impact on the analysis, interpretation and performance of statistical and machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Isolation Forest outlier detector with 100 estimators\n",
    "detector = IsolationForest(n_estimators=100)\n",
    "\n",
    "# Fit the detector to the data and obtain outlier labels\n",
    "out = pd.Series(detector.fit_predict(df), index=df.index)\n",
    "\n",
    "# Identify outliers by mapping -1 labels to True, others to False\n",
    "is_outlier = out.map(lambda x: x == -1)\n",
    "\n",
    "# Create a new column 'is_outlier' in the original DataFrame to mark outliers\n",
    "df_outlier[\"is_outlier\"] = is_outlier\n",
    "\n",
    "# Get the indices of the rows identified as outliers\n",
    "indices = is_outlier.index[is_outlier == True]\n",
    "\n",
    "# Drop rows identified as outliers from the original DataFrame\n",
    "df = df.drop(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Preprocessed Dataset\n",
    "Save the processed data in a new file. Rename if you need multiple files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/processed/processed.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN4Pvx8rIJWWog9ZJOkf8EJ",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "template_env",
   "language": "python",
   "name": "template_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
