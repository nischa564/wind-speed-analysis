{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Data Exploration\n",
    "This notebook allows to visualize the features of the raw dataset using different plots.\n",
    "\n",
    "## Imports and loading\n",
    "Import necessary packages and load the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    ! git clone https://github.com/nischa564/wind-speed-analysis.git # clone repository for colab\n",
    "    ! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pykalman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics.tsaplots import plot_predict\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, explained_variance_score, accuracy_score, balanced_accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import pykalman\n",
    "\n",
    "from dateutil import parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define numeric datatypes\n",
    "NUMERICS = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv file\n",
    "df = pd.read_csv('wind-speed-analysis/data/raw/wind_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the Dataset\n",
    "Shows a few samples of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics about the Data\n",
    "Shows common meta information and statistics of the dataset like datatypes, number of missing values, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape\n",
    "# (#rows, #columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = df.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Line Plot\n",
    "Line plots are commonly used to visualize the trend of a variable over a continuous time. The plot connects data points with straight lines, making it easy to see the overall trend or pattern in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first column as a line plot\n",
    "plt.plot(df.iloc[:, 0], label='<Column 1>')\n",
    "#plt.plot(df.iloc[:, 1], label='<Column 2>')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Feature(s)')\n",
    "plt.title('Line Plot')\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram\n",
    "Histograms are used to represent the distribution of a single variable and show the frequency of different values or ranges. The plot consists of bars where the height of each bar corresponds to the frequency of data within a specified bin or range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram for the first column\n",
    "plt.hist(df.iloc[:, 0], bins=10, alpha=0.5, label='<Column 1>')\n",
    "#plt.hist(df.iloc[:, 1], bins=10, alpha=0.5, label='<Column 2>')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Feature Values')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram')\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Violin Plot\n",
    "Violin plots are useful for visualizing the distribution of a variable or comparing the distributions of multiple variables. The plot combines aspects of box plots and kernel density estimation, providing insights into the distribution, quartiles, and probability density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Violin trace for each column\n",
    "traces = []\n",
    "for column in df.columns:\n",
    "    trace = go.Violin(y=df[column], name=column)\n",
    "    \n",
    "    # Set box_visible to False only if more than 5 features\n",
    "    if len(traces) > 5:\n",
    "        trace.visible = False\n",
    "    \n",
    "    traces.append(trace)\n",
    "\n",
    "# Create the layout\n",
    "layout = go.Layout(title='Violin Plot of Columns', xaxis=dict(title='Columns'), yaxis=dict(title='Values'))\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure(data=traces, layout=layout)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Correlation Map\n",
    "Correlation heatmaps are used to visualize the correlation structure between numeric variables in a dataset. Each cell in the heatmap represents the correlation coefficient between two variables. The color scale typically ranges from cool colors (e.g., blue) for negative correlations to warm colors (e.g., red) for positive correlations. A high positive correlation is represented by a lighter color, while a high negative correlation is represented by a darker color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numeric columns since only them can be shown\n",
    "cols_num = list(df.select_dtypes(include=NUMERICS).columns)\n",
    "# Select the columns that you need\n",
    "df_num = df[cols_num]\n",
    "\n",
    "corr = df_num.corr()\n",
    "fig = px.imshow(corr)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatterplot\n",
    "Scatter plots with color encoding are useful for visualizing the relationship between two variables, where the color represents a third variable. Data points are represented as markers, and the color of each marker encodes information about a third variable, providing insights into multivariate relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select two columns for the scatter plot\n",
    "x_column = '<Column 1>'\n",
    "y_column = '<Column 2>'\n",
    "# Select a color column if you want to do classification\n",
    "#color_column = '<Class Column>'\n",
    "\n",
    "# Create a scatter plot trace\n",
    "scatter_trace = go.Scatter(\n",
    "    x=df[x_column],\n",
    "    y=df[y_column],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=10,\n",
    "#        color=df[color_column],  # Use the values from the ColorColumn for color encoding\n",
    "#        colorscale='Viridis',  # You can choose a different colorscale if needed\n",
    "#        colorbar=dict(title=color_column)\n",
    "    ),\n",
    "    name=f'{x_column} vs {y_column}'\n",
    ")\n",
    "\n",
    "# Create the layout\n",
    "layout = go.Layout(title=f'Scatter Plot of {x_column} vs {y_column}', xaxis=dict(title=x_column), yaxis=dict(title=y_column))\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure(data=[scatter_trace], layout=layout)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Data Preprocessing\n",
    "This notebook allows to clean the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Unwanted Features\n",
    "If you already know you don't need certain features, remove them before the preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns by names\n",
    "#columns_to_drop = ['<Column1>', '<Column3>']\n",
    "#df = df.drop(columns=columns_to_drop, axis=1)\n",
    "\n",
    "# Drop columns by index\n",
    "#index = 0\n",
    "#df = df.drop(df.columns[index], axis=1)\n",
    "\n",
    "# Drop columns by index range\n",
    "#start_index = 0\n",
    "#end_index = 1\n",
    "#df = df.drop(df.columns[start_index:end_index + 1], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Categorical Features\n",
    "The further preprocessing requires only numeric features (no strings). So convert all categorical to numeric features before continuing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding\n",
    "Label Encoding is suitable when the categorical values have an ordinal relationship, meaning there is a meaningful order among the categories. Each category is assigned a unique numerical label. The labels are often assigned in ascending order based on their alphabetical or numerical order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns which should be encoded\n",
    "cols_cat = ['<Column 1>', '<Column 3>']\n",
    "\n",
    "\n",
    "# Loop through each categorical column to perform label encoding\n",
    "for i in cols_cat:\n",
    "    # Step 1: Store the original column values\n",
    "    original = df[i]\n",
    "\n",
    "    # Step 2: Create a mask for missing values in the column\n",
    "    mask = df[i].isnull()\n",
    "\n",
    "    # Step 3: Perform label encoding on the column and replace the original values\n",
    "    df[i] = LabelEncoder().fit_transform(df[i].astype(str))\n",
    "\n",
    "    # Step 4: Replace the encoded values with original values for missing values\n",
    "    df[i] = df[i].where(~mask, original)\n",
    "\n",
    "    # Step 5: Convert the column back to integers, treating 'nan' as NaN\n",
    "    df[i] = df[i].apply(lambda x: int(x) if str(x) != 'nan' else np.nan) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding\n",
    "One-Hot Encoding is suitable when the categorical values are nominal, meaning there is no inherent order among the categories. Each category is represented by a binary column (0 or 1) in a new matrix. The column corresponding to the category is marked with a 1, and others are marked with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and select the columns which should be encoded\n",
    "cols_cat = ['<Column 1>', '<Column 3>']\n",
    "\n",
    "# Define the One Hot Encoder\n",
    "encoder = OneHotEncoder(drop='first', handle_unknown='ignore')\n",
    "\n",
    "# Encode the selected columns\n",
    "df_cat_encoded = encoder.fit_transform(df_cat.astype(str)).toarray()\n",
    "\n",
    "# Save the result in a dataframe\n",
    "df_encoded = pd.DataFrame(df_cat_encoded, index=df_cat.index, columns=encoder.get_feature_names_out(df_cat.columns))\n",
    "\n",
    "# Delete the old features\n",
    "df = df.drop(cols_cat, axis=1)\n",
    "\n",
    "# Concat the onehot features back to the data \n",
    "df = pd.concat([df, df_encoded], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date Encoding\n",
    "Encode cyclic data using sine and cosine functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and select the columns which should be encoded\n",
    "#date_cols = ['<Column 1>']\n",
    "date_cols = ['sepal.width']\n",
    "\n",
    "for col in date_cols:\n",
    "    # Parse the date format\n",
    "    df[col] = df[col].apply(lambda x: parser.parse(x) if isinstance(x, str) else x)\n",
    "\n",
    "    # Encode year linearly\n",
    "    df[col + ' year'] = df[col].dt.year\n",
    "\n",
    "    # Encode other components using sine and cosine functions\n",
    "    components = ['month', 'day', 'hour', 'minute', 'second', 'microsecond']\n",
    "    for comp in components:\n",
    "        df[col + ' ' + comp + ' sin'] = np.sin(2 * math.pi * df[col].dt.__getattribute__(comp) / df[col].dt.__getattribute__(comp).max())\n",
    "        df[col + ' ' + comp + ' cos'] = np.cos(2 * math.pi * df[col].dt.__getattribute__(comp) / df[col].dt.__getattribute__(comp).max())\n",
    "\n",
    "# Remove the original date columns\n",
    "df.drop(date_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Already Numeric\n",
    "Some columns consist already of numeric values. Just convert them to numeric values. If the decimal numbers use the german writing, replace the comma with points before converting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_column_comma_and_set_type_float(col: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Converts a Pandas Series containing numeric strings with commas to float values.\n",
    "\n",
    "    Parameters:\n",
    "    - col: The input Pandas Series containing numeric strings.\n",
    "\n",
    "    Returns:\n",
    "    - The converted Pandas Series with values converted to float.\n",
    "    \"\"\"\n",
    "    # Use the map function to apply the specified lambda function to each element in the column\n",
    "    col = col.map(lambda x: x.replace('.', '0.0').replace(',', '.') if type(x) != float else x)\n",
    "\n",
    "    # Convert the column to the float type\n",
    "    col = col.astype(float)\n",
    "\n",
    "    # Return the converted column\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns which should be encoded\n",
    "cols_cat = ['<Column 1>', '<Column 3>']\n",
    "\n",
    "# Loop through each selected categorical column\n",
    "for i in cols_cat:\n",
    "    # Check if the column contains '.' or ',' in any of its values\n",
    "    if df[i].str.contains('.').any() or df[i].str.contains(',').any():\n",
    "        # If yes, apply the custom function to convert the column to float\n",
    "        df[i] = convert_column_comma_and_set_type_float(df[i])\n",
    "    else:\n",
    "        # If no '.', ',' found, use pd.to_numeric to convert the column to numeric\n",
    "        df[i] = pd.to_numeric(df[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill Missing Values\n",
    "Further operations require a dataset without missing values. So fill all missing values before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the imputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "# Apply the imputation to the dataset\n",
    "df = df.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect and Remove Outlier\n",
    "Outlier detection is important in various fields and applications because outliers, which are data points that significantly differ from the majority of the data, can have a significant impact on the analysis, interpretation and performance of statistical and machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Isolation Forest outlier detector with 100 estimators\n",
    "detector = IsolationForest(n_estimators=100)\n",
    "\n",
    "# Fit the detector to the data and obtain outlier labels\n",
    "out = pd.Series(detector.fit_predict(df), index=df.index)\n",
    "\n",
    "# Identify outliers by mapping -1 labels to True, others to False\n",
    "is_outlier = out.map(lambda x: x == -1)\n",
    "\n",
    "# Create a new column 'is_outlier' in the original DataFrame to mark outliers\n",
    "df_outlier[\"is_outlier\"] = is_outlier\n",
    "\n",
    "# Get the indices of the rows identified as outliers\n",
    "indices = is_outlier.index[is_outlier == True]\n",
    "\n",
    "# Drop rows identified as outliers from the original DataFrame\n",
    "df = df.drop(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Preprocessed Dataset\n",
    "Save the processed data in a new file. Rename if you need multiple files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('wind-speed-analysis/data/processed/processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 - Data Transformation\n",
    "This notebook is for creating one/multiple datasets with different feature subsets and transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Unwanted Features\n",
    "Often only a subset of features is required. So delete the rest of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns by names\n",
    "#columns_to_drop = ['<Column1>', '<Column3>']\n",
    "#df = df.drop(columns=columns_to_drop, axis=1)\n",
    "\n",
    "# Drop columns by index\n",
    "#index = 0\n",
    "#df = df.drop(df.columns[index], axis=1)\n",
    "\n",
    "# Drop columns by index range\n",
    "#start_index = 0\n",
    "#end_index = 1\n",
    "#df = df.drop(df.columns[start_index:end_index + 1], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "Normalization scales data to a standard range, usually between 0 and 1. It is useful when the features have different scales and ensures that all features contribute equally to the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and select the columns which should be normalized\n",
    "#cols = ['<Column 1>', '<Column 3>']\n",
    "cols = list(df.columns)\n",
    "\n",
    "# Create a MinMaxScaler object\n",
    "scaler = MinMaxScaler()\n",
    "    \n",
    "# Fit the scaler to the selected columns and transform them\n",
    "data_normalized = scaler.fit_transform(df[cols].values)\n",
    "    \n",
    "# Convert the normalized data to a pandas dataframe\n",
    "df_normalized = pd.DataFrame(data_normalized, index=df.index, columns=cols)\n",
    "    \n",
    "# Concatenate the normalized columns with the unnormalized columns\n",
    "df_untransformed = df[[col for col in df.columns if col not in cols]]\n",
    "df = pd.concat([df_untransformed, df_normalized], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization\n",
    "Standardization transforms data to have a mean of 0 and a standard deviation of 1. It is effective when features have different scales and you can assume a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and select the columns which should be standardized\n",
    "#cols = ['<Column 1>', '<Column 3>']\n",
    "cols = list(df.columns)\n",
    "\n",
    "# Create a StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "    \n",
    "# Fit the scaler to the selected columns and transform them\n",
    "data_standardized = scaler.fit_transform(df[cols].values)\n",
    "    \n",
    "# Convert the standardized data to a pandas dataframe\n",
    "df_standardized = pd.DataFrame(data_standardized, index=df.index, columns=cols)\n",
    "    \n",
    "# Concatenate the standardized columns with the unselected columns\n",
    "df_untransformed = df[[col for col in df.columns if col not in cols]]\n",
    "df = pd.concat([df_untransformed, df_standardized], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA\n",
    "PCA is a dimensionality reduction technique that transforms data into a new set of uncorrelated variables (principal components). It is used to capture the most significant variability in the data while reducing its dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and select the columns on which the pca is applied\n",
    "#cols = ['<Column 1>', '<Column 3>']\n",
    "cols = list(df.columns)\n",
    "\n",
    "# Fit the PCA and transform on the selected columns\n",
    "pca = PCA(n_components=2)\n",
    "data_pca = pca.fit_transform(df[cols])\n",
    "\n",
    "# Define a new name for the new features\n",
    "feature_name = 'pca_feature'\n",
    "\n",
    "# Convert the PCA data to a pandas dataframe\n",
    "new_cols = [f'{feature_name}_' + str(i+1) for i in range(data_pca.shape[1])]\n",
    "df_pca = pd.DataFrame(data_pca, columns=new_cols, index=df.index)\n",
    "\n",
    "# Concatenate the pca columns with the unselected columns\n",
    "df_untransformed = df[[col for col in df.columns if col not in cols]]\n",
    "df = pd.concat([df_pca, df_untransformed], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shifting\n",
    "Shifting involves moving data points by a constant value. It is used for various purposes, such as aligning signals or adjusting time series for temporal considerations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the number of periods to be shifted\n",
    "periods = 1\n",
    "# Select if you want to have all shifts between 1 and periods as own column\n",
    "multi_shift = True\n",
    "    \n",
    "# Initialize an empty dataframe to store the windowed data\n",
    "df_shifted = pd.DataFrame()\n",
    "    \n",
    "# Loop through each column in the dataframe and create the requested shifts for the specified columns\n",
    "for col in df.columns:\n",
    "    if col in cols:\n",
    "        if multi_shift:\n",
    "            for i in range(1, periods+1):\n",
    "                # define the name for the shifted column\n",
    "                shifted_col_name = col + '_shifted_' + str(i)\n",
    "                df_shifted[shifted_col_name] = df[col].shift(i)\n",
    "        else:\n",
    "            # define the name for the shifted column\n",
    "            shifted_col_name = col + '_shifted_' + str(periods)\n",
    "            df_shifted[shifted_col_name] = df[col].shift(periods)\n",
    "    \n",
    "# Convert the shifted data to a pandas dataframe\n",
    "new_cols = list(df_shifted.columns)\n",
    "df = pd.concat([df_shifted, df], axis=1)\n",
    "\n",
    "# Fill na values which are created during the process\n",
    "df = df.backfill()\n",
    "df = df.ffill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sliding Window\n",
    "A Sliding Window extracts subsets of data points sequentially, creating a \"window\" that moves through the dataset. It is used for tasks like feature extraction or smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and select the columns on which the sliding window is applied\n",
    "#cols = ['<Column 1>', '<Column 3>']\n",
    "cols = list(df.columns)\n",
    "\n",
    "# Selet a window size\n",
    "window_size = 2\n",
    "# Select between 'sum', 'mean', 'median', 'min', 'max', 'std' \n",
    "operation = 'mean' \n",
    "    \n",
    "# Initialize an empty dataframe to store the windowed data\n",
    "df_windowed = pd.DataFrame()\n",
    "    \n",
    "# Iterate over each column in the dataframe\n",
    "for col in cols:\n",
    "        \n",
    "    # Apply the specified operations to the column using a rolling window\n",
    "    if 'sum' == operation:\n",
    "        df_windowed[f'{col}_sum'] = df[col].rolling(window_size).sum()\n",
    "    elif 'mean' == operation:\n",
    "        df_windowed[f'{col}_mean'] = df[col].rolling(window_size).mean()\n",
    "    elif 'median' == operation:\n",
    "        df_windowed[f'{col}_median'] = df[col].rolling(window_size).median()\n",
    "    elif 'min' == operation:\n",
    "        df_windowed[f'{col}_min'] = df[col].rolling(window_size).min()\n",
    "    elif 'max' == operation:\n",
    "        df_windowed[f'{col}_max'] = df[col].rolling(window_size).max()\n",
    "    elif 'std' == operation:\n",
    "        df_windowed[f'{col}_std'] = df[col].rolling(window_size).std()\n",
    "        \n",
    "# Add the windowed data to the windowed dataframe\n",
    "new_cols = list(df_windowed.columns)\n",
    "df_windowed = pd.concat([df_windowed, df], axis=1)\n",
    "\n",
    "# Drop old columns\n",
    "df = df_windowed.drop(cols, axis=1)\n",
    "\n",
    "# Fill na values which are created during the process\n",
    "df = df.backfill()\n",
    "df = df.ffill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differencing\n",
    "It calculates the difference between consecutive data points. It is often used to transform a time series into a stationary series for trend and seasonality removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and select the columns on which the sliding window is applied\n",
    "#cols = ['<Column 1>', '<Column 3>']\n",
    "cols = list(df.columns)\n",
    "\n",
    "periods = 1\n",
    "    \n",
    "# Initialize an empty dataframe to store the windowed data\n",
    "df_diff = pd.DataFrame()\n",
    "    \n",
    "# Iterate over each column in the dataframe\n",
    "for col in cols:\n",
    "    # Apply the specified operations to the column using a rolling window\n",
    "    df_diff[f'{col}_diff'] = df[col].diff(periods=periods)\n",
    "    \n",
    "# Add the windowed data to the windowed dataframe\n",
    "new_cols = list(df_diff.columns)\n",
    "df_diff = pd.concat([df_diff, df], axis=1)\n",
    "\n",
    "# Drop old columns\n",
    "df = df_diff.drop(cols, axis=1)\n",
    "\n",
    "# Fill na values which are created during the process\n",
    "df = df.backfill()\n",
    "df = df.ffill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kalman Filter\n",
    "A Kalman Filter estimates the state of a dynamic system from a series of noisy measurements. It is widely used in signal processing, control systems and sensor fusion applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and select the columns on which the kalman filter is applied\n",
    "#cols = ['<Column 1>', '<Column 3>']\n",
    "cols = list(df.columns)\n",
    "\n",
    "# Initialize the Kalman filter\n",
    "kf = pykalman.KalmanFilter()\n",
    "    \n",
    "# Create an empty dataframe to store the filtered data\n",
    "df_filtered = pd.DataFrame(index=df.index)\n",
    "    \n",
    "# Iterate over each column to be filtered\n",
    "for col in cols:\n",
    "        \n",
    "    # Get the time series data as a numpy array\n",
    "    data = df[col].values\n",
    "        \n",
    "    # Apply the Kalman filter to the data\n",
    "    data_filtered, _ = kf.filter(data)\n",
    "        \n",
    "    # Add the filtered data to the filtered dataframe\n",
    "    df_filtered[col] = data_filtered\n",
    "        \n",
    "# Concatenate the pca columns with the unselected columns\n",
    "df_untransformed = df[[col for col in df.columns if col not in cols]]\n",
    "df = pd.concat([df_filtered, df_untransformed], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Preprocessed Dataset\n",
    "Save the transformed data in a new file. Rename if you need multiple files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('wind-speed-analysis/data/transformed/transformed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PoxkFVyPmKMJ"
   },
   "source": [
    "# 4.0 - Data Analysis\n",
    "This notebook is for analysing the transformed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 295223,
     "status": "ok",
     "timestamp": 1703770251981,
     "user": {
      "displayName": "Nischa564",
      "userId": "09387795752635677771"
     },
     "user_tz": -60
    },
    "id": "iF1MPPTSmVVY",
    "outputId": "42416b57-c726-42e0-9e49-361df2b73313"
   },
   "source": [
    "## Imports and loading\n",
    "Import necessary packages and load the transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list to save the results\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "# Define a list to name the different models\n",
    "models = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data in Trainset and Testset\n",
    "In order to train and evaluate the model, we need a train set and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and y\n",
    "y = df['<Target Column>']\n",
    "X = df.drop(columns=['<Target Column>'])\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Get train and test size\n",
    "train_size = len(y_train)\n",
    "test_size = len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Baseline\n",
    "A baseline model is a simple, often naive, model that serves as a point of reference for evaluating the performance of more sophisticated machine learning models. A baseline model provides a benchmark against which the performance of more complex models can be compared. It serves as a starting point for assessing the effectiveness of your machine learning solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name the model\n",
    "models.append('Simple Baseline')\n",
    "\n",
    "# Define the dummy regressor\n",
    "dr = DummyRegressor(strategy='mean')\n",
    "\n",
    "# Fit the model\n",
    "dr.fit(X_train, y_train)\n",
    "\n",
    "# Get train and test prediction\n",
    "pred_train = dr.predict(X_train)\n",
    "pred_test = dr.predict(X_test)\n",
    "\n",
    "# Compute the score\n",
    "train_score = mean_absolute_error(y_train, pred_train)\n",
    "test_score = mean_absolute_error(y_test, pred_test)\n",
    "\n",
    "# Add score to list\n",
    "train_scores.append(train_score)\n",
    "test_scores.append(test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the test prediction\n",
    "fig, ax = plt.subplots()\n",
    "ax = df['<Target Column>'].plot(ax=ax)\n",
    "plt.plot(pred_test, label='Baseline Prediction')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARIMA Baseline\n",
    "ARIMA is designed to handle univariate time series data and is effective for capturing and forecasting temporal patterns in a dataset. It is a versatile model that combines autoregression, differencing, and moving averages to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name the model\n",
    "models.append('ARIMA')\n",
    "\n",
    "# Define the order\n",
    "order = (0,1,0)\n",
    "\n",
    "# Define the ARIMA model\n",
    "model = sm.tsa.arima.ARIMA(df['<Target Column>'], order=order)\n",
    "\n",
    "# Fit the model\n",
    "results = model.fit()\n",
    "\n",
    "# Get train and test prediction\n",
    "pred_train = results.predict(end=train_size-1)\n",
    "pred_test = results.predict(start=train_size)\n",
    "\n",
    "# Compute the score\n",
    "train_score = mean_absolute_error(y_train, pred_train)\n",
    "test_score = mean_absolute_error(y_test, pred_test)\n",
    "\n",
    "# Add score to list\n",
    "train_scores.append(train_score)\n",
    "test_scores.append(test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the prediction\n",
    "fig, ax = plt.subplots()\n",
    "ax = df['<Target Column'].plot(ax=ax)\n",
    "plot_predict(results, train_size, train_size + test_size - 1, ax=ax)\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel(f'Target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Evaluate ML Models\n",
    "Train and evaluate different models with different hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name the model\n",
    "models.append('Linear Regression')\n",
    "\n",
    "# Define a linear regression\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Fit the model\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Get train and test prediction\n",
    "pred_train = lr.predict(X_train)\n",
    "pred_test = lr.predict(X_test)\n",
    "\n",
    "# Compute the score\n",
    "train_score = mean_absolute_error(y_train, pred_train)\n",
    "test_score = mean_absolute_error(y_test, pred_test)\n",
    "\n",
    "# Add score to list\n",
    "train_scores.append(train_score)\n",
    "test_scores.append(test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the test prediction\n",
    "fig, ax = plt.subplots()\n",
    "ax = df['<Target Column'].plot(ax=ax)\n",
    "plt.plot(pred_test, label='Linear Regression Prediction')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name the model\n",
    "models.append('Decision Tree')\n",
    "\n",
    "# Define a decision tree\n",
    "dt = DecisionTreeRegressor(criterion='squared_error', max_depth=None)\n",
    "\n",
    "# Fit the model\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Get train and test prediction\n",
    "pred_train = dt.predict(X_train)\n",
    "pred_test = dt.predict(X_test)\n",
    "\n",
    "# Compute the score\n",
    "train_score = mean_absolute_error(y_train, pred_train)\n",
    "test_score = mean_absolute_error(y_test, pred_test)\n",
    "\n",
    "# Add score to list\n",
    "train_scores.append(train_score)\n",
    "test_scores.append(test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the test prediction\n",
    "fig, ax = plt.subplots()\n",
    "ax = df['<Target Column'].plot(ax=ax)\n",
    "plt.plot(pred_test, label='Decision Tree Prediction')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name the model\n",
    "models.append('Random Forest')\n",
    "\n",
    "# Define a random forest\n",
    "rf = RandomForestRegressor(n_estimators=100, criterion='squared_error', max_depth=None)\n",
    "\n",
    "# Fit the model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Get train and test prediction\n",
    "pred_train = dt.predict(X_train)\n",
    "pred_test = dt.predict(X_test)\n",
    "\n",
    "# Compute the score\n",
    "train_score = mean_absolute_error(y_train, pred_train)\n",
    "test_score = mean_absolute_error(y_test, pred_test)\n",
    "\n",
    "# Add score to list\n",
    "train_scores.append(train_score)\n",
    "test_scores.append(test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the test prediction\n",
    "fig, ax = plt.subplots()\n",
    "ax = df['<Target Column'].plot(ax=ax)\n",
    "plt.plot(pred_test, label='Random Forest Prediction')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(models))\n",
    "width = 0.3\n",
    "\n",
    "plt.bar(x - 0.17, train_scores, width, label='Train')\n",
    "plt.bar(x + 0.17, test_scores, width, label='Test')\n",
    "plt.xticks(ticks=x, labels=models, rotation=45)\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel(f'Scores')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN4Pvx8rIJWWog9ZJOkf8EJ",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "template_env",
   "language": "python",
   "name": "template_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
